<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Isaac Development Team</title>
  </head>
  <body>
    <big><big><b>Development Team</b></big></big><b><br>
    </b><br>
    Currently, the development team consists of me:<br>
    <br>
    <br>
    Docent Dr. Manfred G. Grabherr<br>
    <img alt="Image" title="Image"
      src="MG_3961_small1.jpg"
      height="115" width="76"><br>
    <br>
    I have been programming since I was 13. Not that I wouldn't have
    done it earlier, but back then, it was difficult to get a hold of a
    computer. Unsurprisingly, the first complete software program that
    my friends and I wrote together was a game. A 'physics game',
    actually, in which the player had to land a multistage rocket on the
    moon, manually controlling the entire flight and getting constant
    feedback from a rather simple physics engine. No graphics involved,
    and in the end, none of us ever managed to land safely on the moon.
    Which might have been not too bad after all, since we never planned
    for any return trip. That would have taken us another couple of
    months, since we could only use the computer on Wednesday afternoons
    at school.<br>
    <br>
    There is something incredibly fascinating about the combination of
    computer games and modeling reality, be it physics, graphics, sound,
    or behavior. It brings together very different aspects of human
    endeavors; exploration, achievement, interaction, competition, and
    perhaps most importantly: the awe of seeing something beautiful <i>work</i>.
    Granted, these elements can also be seen as integral parts of any
    software, but what sets games apart is that we play them for the
    pure fun of it, for the experience alone, and while we do that, we
    learn many things about the world - and perhaps about ourselves as
    well - without even noticing. And, having been an engineer and
    scientist for decades, it is a pleasure to watch kids teaching
    themselves <a
      href="http://en.wikipedia.org/wiki/Java_%28programming_language%29">Java</a>
    or <a href="http://en.wikipedia.org/wiki/Lua">Lua</a> so that they
    can write <a
      href="http://en.wikipedia.org/wiki/Mod_%28video_gaming%29">mods</a>
    for sandbox games such as <a href="https://minecraft.net/">Minecraft</a>
    or <a href="http://www.garrysmod.com/">Garry's Mod</a>. These might
    be the engineers and scientists of the future, and I think the
    outlook is more than bright.<br>
    <br>
    But for now, there is one particular area in which further
    development is desired. Computers have come a long way since I was
    13, and now even my phone is orders of magnitudes more powerful than
    the largest machines that existed back then. This opens up
    possibilities that we could not have imagined only a few decades
    ago, and one of them is <i>modeling cognition</i>. Consider the
    following: in the scientific area that I am working in,
    computational genomics, advances in biochemistry have made it
    possible to generate enormously large data sets, for example the
    whole genomes of dozens or even hundreds of individuals, or the tens
    of thousands of genes that they express. To put this in perspective,
    the human genome contains about 3 billion <a
      href="http://en.wikipedia.org/wiki/Nucleotide">nucleotides</a>,
    represented by the letters A, C, G, and T. Multiply this number by
    the number of individuals you sequence, and it is easy to see that
    the amount of data is, well, large. And in there, you now want to
    know which nucleotides make some <a
      href="http://www.ncbi.nlm.nih.gov/pubmed/24948738">crows black and
      others grey</a>, how <a
      href="http://www.ncbi.nlm.nih.gov/pubmed/22481358">stickleback
      fish</a> have adapted to different environments, how <a
      href="http://www.ncbi.nlm.nih.gov/pubmed/25686609">Darwin's
      finches change their beak shapes</a>, or what happens in the
    pancreas of people <a
      href="http://www.ncbi.nlm.nih.gov/pubmed/25677915">with type I
      diabetes</a>.<br>
    <br>
    The classic approach, which is to employ hundreds of graduate
    students to manually look through the data, is not always feasible
    for these data sets. This is why <a
      href="http://en.wikipedia.org/wiki/Machine_learning">machine
      learning</a>, i.e. the concept of "let the computer figure it out
    by itself", has become very popular within the past few years in
    this area. What this approach does, in essence, is to identify
    distinct patterns in large data sets, partition the space
    accordingly, and, in some cases, propose models that describe the
    data. Computationallly, this methodology is by no means cheap, but
    with proper parallelization and a modern compute architecture, it is
    very well feasible to process tons and tons of data in reasonable
    time frames.<br>
    <br>
    Let us now perhaps scale back the amount of data, but let us
    increase the complexity. While some <a
      href="http://www.biomedcentral.com/1471-2164/14/347">genomic
      analysis software</a> already has the concept of <i>space</i>,
    what happens if we add a component that deals with <i>time</i>?
    What we get is a system that automatically recognizes patterns in
    both time and space, and if we add another component that takes into
    account <i>interaction</i> and the <i>consequences</i> thereof,
    then these patterns can also be <i>qualified</i>. In other words,
    we now have a machine that <i>remembers,</i> <i>learns,</i> and
    determines its own <i>motivations.</i> It is not only able to
    follow ongoing action, but constantly comparing what is happening
    with its memory allows it to anticipate the future. It projects what
    will happen from its experience. Likewise, it can "think" through
    possible chains of actions and choose the best - or least worst -
    option, while able to revise strategy if things do not go according
    to plan. In short: now we have a system that models cognition, even
    if it is on a crude level compared to humans. But, if we take this
    system and let it control the non-playable characters in a computer
    game, even if these are just as smart as ants or chickens, then we
    get something that adds a whole level to game play. <br>
    <br>
    At least, this is what I am convinced of. And, as always, there is
    only one way to find out.<br>
    <br>
    <br>
    <br>
    <br>
  </body>
</html>
